I changed the --ntasks flag on the input to BertLarge.sh from the default value of 16 to 8 and later to 32. I ran into unexpected assertion errors about throughput in both cases. It made sense for the ntasks = 8 case because expected value was at least 560000, and the value found was 302273. However, in the ntasks = 32 case the expected value was at least 620000, and the value found was 1134548, which is greater than 620000. I'm a bit confused about that. 

But leaving that aside, the code still seemed to work for training purposes before crashing at the last minute due to the said assertion mentioned above. 

Training time for all cases were very similar. Training sequences per second roughly doubled with doubling number of ntasks, as did training samples per second. However, the final loss for all cases remain quite similar, around ~8.2 in all cases. Increasing ntasks seem to have a very small effect on the final loss, as it changed from 8.309 to 8.257 to 8.228 when going from 8 to 16 to 32 in ntasks. 