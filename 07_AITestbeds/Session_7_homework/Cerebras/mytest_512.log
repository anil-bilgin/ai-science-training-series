2024-04-06 21:05:55,325 INFO:   Effective batch size is 512.
2024-04-06 21:05:55,348 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-04-06 21:05:55,349 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-04-06 21:05:55,349 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-04-06 21:05:56,632 INFO:   Saving checkpoint at step 0
2024-04-06 21:06:23,689 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-04-06 21:06:37,763 INFO:   Compiling the model. This may take a few minutes.
2024-04-06 21:06:37,765 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-06 21:06:38,956 INFO:   Initiating a new image build job against the cluster server.
2024-04-06 21:06:39,064 INFO:   Custom worker image build is disabled from server.
2024-04-06 21:06:39,070 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-06 21:06:39,403 INFO:   Initiating a new compile wsjob against the cluster server.
2024-04-06 21:06:39,519 INFO:   compile job id: wsjob-myh7r35mowl3rccytwxnbo, remote log path: /n1/wsjob/workdir/job-operator/wsjob-myh7r35mowl3rccytwxnbo
2024-04-06 21:06:49,562 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-06 21:07:19,572 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-06 21:07:23,544 INFO:   Pre-optimization transforms...
2024-04-06 21:07:29,445 INFO:   Optimizing layouts and memory usage...
2024-04-06 21:07:29,490 INFO:   Gradient accumulation enabled
2024-04-06 21:07:29,491 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.

2024-04-06 21:07:29,493 INFO:   Gradient accumulation trying sub-batch size 8...
2024-04-06 21:07:34,653 INFO:   Exploring floorplans
2024-04-06 21:07:42,008 INFO:   Exploring data layouts
2024-04-06 21:07:53,754 INFO:   Optimizing memory usage
2024-04-06 21:08:40,450 INFO:   Gradient accumulation trying sub-batch size 64...
2024-04-06 21:08:45,743 INFO:   Exploring floorplans
2024-04-06 21:08:54,002 INFO:   Exploring data layouts
2024-04-06 21:09:12,097 INFO:   Optimizing memory usage
2024-04-06 21:09:45,025 INFO:   Gradient accumulation trying sub-batch size 32...
2024-04-06 21:09:50,116 INFO:   Exploring floorplans
2024-04-06 21:09:56,891 INFO:   Exploring data layouts
2024-04-06 21:10:12,819 INFO:   Optimizing memory usage
2024-04-06 21:10:48,196 INFO:   Gradient accumulation trying sub-batch size 128...
2024-04-06 21:10:53,748 INFO:   Exploring floorplans
2024-04-06 21:11:04,861 INFO:   Exploring data layouts
2024-04-06 21:11:24,144 INFO:   Optimizing memory usage
2024-04-06 21:11:52,862 INFO:   Gradient accumulation trying sub-batch size 256...
2024-04-06 21:11:59,368 INFO:   Exploring floorplans
2024-04-06 21:12:15,608 INFO:   Exploring data layouts
2024-04-06 21:12:41,594 INFO:   Optimizing memory usage
2024-04-06 21:13:25,231 INFO:   Exploring floorplans
2024-04-06 21:13:28,533 INFO:   Exploring data layouts
2024-04-06 21:14:02,063 INFO:   Optimizing memory usage
2024-04-06 21:14:35,076 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 512 with 6 lanes

2024-04-06 21:14:35,123 INFO:   Post-layout optimizations...
2024-04-06 21:14:48,530 INFO:   Allocating buffers...
2024-04-06 21:14:51,002 INFO:   Code generation...
2024-04-06 21:15:03,854 INFO:   Compiling image...
2024-04-06 21:15:03,859 INFO:   Compiling kernels
2024-04-06 21:18:23,159 INFO:   Compiling final image
2024-04-06 21:21:03,070 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_8939750200954608837
2024-04-06 21:21:03,120 INFO:   Heartbeat thread stopped for wsjob-myh7r35mowl3rccytwxnbo.
2024-04-06 21:21:03,123 INFO:   Compile was successful!
2024-04-06 21:21:03,128 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-04-06 21:21:05,457 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-06 21:21:05,792 INFO:   Initiating a new execute wsjob against the cluster server.
2024-04-06 21:21:05,919 INFO:   execute job id: wsjob-mdys2retqurzdf8ty2ig68, remote log path: /n1/wsjob/workdir/job-operator/wsjob-mdys2retqurzdf8ty2ig68
2024-04-06 21:21:15,962 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 execute job(s) running using 1 system(s), 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.
2024-04-06 21:21:25,940 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-04-06 21:21:35,960 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-06 21:21:56,002 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-06 21:21:56,144 INFO:   Preparing to execute using 1 CSX
2024-04-06 21:22:27,399 INFO:   About to send initial weights
2024-04-06 21:23:00,333 INFO:   Finished sending initial weights
2024-04-06 21:23:00,335 INFO:   Finalizing appliance staging for the run
2024-04-06 21:23:00,355 INFO:   Waiting for device programming to complete
2024-04-06 21:25:23,071 INFO:   Device programming is complete
2024-04-06 21:25:24,019 INFO:   Using network type: ROCE
2024-04-06 21:25:24,020 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-04-06 21:25:24,042 INFO:   Input workers have begun streaming input data
2024-04-06 21:25:40,786 INFO:   Appliance staging is complete
2024-04-06 21:25:40,791 INFO:   Beginning appliance run
2024-04-06 21:25:58,033 INFO:   | Train Device=CSX, Step=100, Loss=9.39062, Rate=2981.15 samples/sec, GlobalRate=2981.15 samples/sec
2024-04-06 21:26:15,577 INFO:   | Train Device=CSX, Step=200, Loss=8.70312, Rate=2943.48 samples/sec, GlobalRate=2949.42 samples/sec
2024-04-06 21:26:33,007 INFO:   | Train Device=CSX, Step=300, Loss=7.79688, Rate=2939.89 samples/sec, GlobalRate=2945.44 samples/sec
2024-04-06 21:26:50,625 INFO:   | Train Device=CSX, Step=400, Loss=7.39062, Rate=2919.61 samples/sec, GlobalRate=2935.50 samples/sec
2024-04-06 21:27:08,257 INFO:   | Train Device=CSX, Step=500, Loss=7.80469, Rate=2910.15 samples/sec, GlobalRate=2929.12 samples/sec
2024-04-06 21:27:25,868 INFO:   | Train Device=CSX, Step=600, Loss=7.53125, Rate=2908.35 samples/sec, GlobalRate=2925.43 samples/sec
2024-04-06 21:27:43,414 INFO:   | Train Device=CSX, Step=700, Loss=7.35156, Rate=2914.23 samples/sec, GlobalRate=2924.39 samples/sec
2024-04-06 21:28:00,931 INFO:   | Train Device=CSX, Step=800, Loss=7.27344, Rate=2919.37 samples/sec, GlobalRate=2924.19 samples/sec
2024-04-06 21:28:18,331 INFO:   | Train Device=CSX, Step=900, Loss=7.35938, Rate=2933.28 samples/sec, GlobalRate=2926.22 samples/sec
2024-04-06 21:28:35,694 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=2942.58 samples/sec, GlobalRate=2928.46 samples/sec
2024-04-06 21:28:35,695 INFO:   Saving checkpoint at step 1000
2024-04-06 21:29:10,952 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-04-06 21:29:48,526 INFO:   Heartbeat thread stopped for wsjob-mdys2retqurzdf8ty2ig68.
2024-04-06 21:29:48,532 INFO:   Training completed successfully!
2024-04-06 21:29:48,532 INFO:   Processed 512000 sample(s) in 174.835966608 seconds.
